{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ca81327e",
      "metadata": {},
      "source": [
        "# InstantID: Zero-shot Identity-Preserving Generation in Seconds\n",
        "\n",
        "Este script implementa [InstantID](https://github.com/InstantX/InstantID), un m√©todo para generar im√°genes \n",
        "que preservan la identidad de una persona en segundos.\n",
        "\n",
        "‚ö†Ô∏è **Importante**: Aseg√∫rate de seleccionar un entorno de ejecuci√≥n con GPU: Runtime -> Change runtime type -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950b3b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar que tenemos GPU disponible\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
        "print(f\"Dispositivo CUDA: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No hay GPU'}\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"No se detect√≥ GPU. Por favor, selecciona un entorno de ejecuci√≥n con GPU: Runtime -> Change runtime type -> GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1dd6d77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clonar el repositorio\n",
        "!git clone https://github.com/krowork/INSTID.git\n",
        "%cd INSTID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0802e6e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî• Instalaci√≥n compatible con PyTorch 2.6+ y Colab actual\n",
        "\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"\\n‚ùå Este notebook requiere una GPU. Por favor, selecciona: Runtime -> Change runtime type -> GPU\")\n",
        "\n",
        "print(\"üîß Iniciando instalaci√≥n compatible con PyTorch 2.6+...\")\n",
        "print(\"‚ú® Usando versiones compatibles con el entorno actual de Colab\")\n",
        "\n",
        "# Verificar versiones actuales\n",
        "print(f\"\\nüìã PyTorch actual: {torch.__version__}\")\n",
        "print(f\"üìã CUDA actual: {torch.version.cuda}\")\n",
        "\n",
        "# Configurar variables de entorno para evitar conflictos\n",
        "import os\n",
        "os.environ['TORCH_HOME'] = './torch_home'\n",
        "os.environ['HF_HOME'] = './hf_home'\n",
        "os.environ['TRANSFORMERS_CACHE'] = './transformers_cache'\n",
        "\n",
        "# Crear directorios de cach√©\n",
        "!mkdir -p ./torch_home ./hf_home ./transformers_cache\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Actualizando dependencias principales...\")\n",
        "# Usar versiones compatibles con PyTorch 2.6+\n",
        "!pip install --quiet --no-warn-script-location transformers>=4.41.0\n",
        "!pip install --quiet --no-warn-script-location diffusers>=0.30.0\n",
        "!pip install --quiet --no-warn-script-location huggingface-hub>=0.25.0\n",
        "!pip install --quiet --no-warn-script-location accelerate>=0.30.0\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Instalando dependencias de visi√≥n...\")\n",
        "!pip install --quiet --no-warn-script-location opencv-python\n",
        "!pip install --quiet --no-warn-script-location Pillow\n",
        "!pip install --quiet --no-warn-script-location safetensors\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Instalando dependencias de IA facial...\")\n",
        "!pip install --quiet --no-warn-script-location insightface\n",
        "!pip install --quiet --no-warn-script-location onnx\n",
        "!pip install --quiet --no-warn-script-location onnxruntime-gpu\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ Instalando ControlNet y utilidades...\")\n",
        "!pip install --quiet --no-warn-script-location controlnet_aux\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ Instalando interfaz web...\")\n",
        "!pip install --quiet --no-warn-script-location gradio\n",
        "\n",
        "print(\"\\n6Ô∏è‚É£ Instalando dependencias adicionales...\")\n",
        "!pip install --quiet --no-warn-script-location psutil  # Para monitoreo de memoria\n",
        "\n",
        "# Verificaci√≥n final\n",
        "print(\"\\nüîç Verificando instalaci√≥n...\")\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
        "    \n",
        "    import diffusers\n",
        "    print(f\"‚úÖ Diffusers: {diffusers.__version__}\")\n",
        "    \n",
        "    import huggingface_hub\n",
        "    print(f\"‚úÖ HuggingFace Hub: {huggingface_hub.__version__}\")\n",
        "    \n",
        "    import accelerate\n",
        "    print(f\"‚úÖ Accelerate: {accelerate.__version__}\")\n",
        "    \n",
        "    import cv2\n",
        "    print(f\"‚úÖ OpenCV: {cv2.__version__}\")\n",
        "    \n",
        "    import insightface\n",
        "    print(f\"‚úÖ InsightFace: {insightface.__version__}\")\n",
        "    \n",
        "    import gradio\n",
        "    print(f\"‚úÖ Gradio: {gradio.__version__}\")\n",
        "    \n",
        "    import psutil\n",
        "    print(f\"‚úÖ PSUtil: {psutil.__version__}\")\n",
        "    \n",
        "    print(f\"\\nüéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üéÆ Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en verificaci√≥n: {e}\")\n",
        "\n",
        "print(\"\\nüéâ ¬°Instalaci√≥n completada exitosamente!\")\n",
        "print(\"\\n‚ö†Ô∏è IMPORTANTE: Reinicia el runtime ahora\")\n",
        "print(\"üí° Runtime ‚Üí Restart runtime\")\n",
        "print(\"\\nDespu√©s de reiniciar, ejecuta la siguiente celda para verificar.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60a4a567",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suprimir advertencias no cr√≠ticas\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "print(\"üîç Verificando instalaci√≥n...\")\n",
        "\n",
        "# Verificar versiones base\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(\"üìã Versiones b√°sicas:\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "\n",
        "# Verificar GPU\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"‚ùå No se detect√≥ GPU. Este notebook requiere una GPU para funcionar.\")\n",
        "\n",
        "print(f\"üéÆ GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"üéÆ Memoria GPU total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "print(f\"üéÆ Memoria GPU disponible: {torch.cuda.mem_get_info()[0] / 1024**3:.1f} GB\")\n",
        "print(f\"üéÆ CUDA versi√≥n: {torch.version.cuda}\")\n",
        "print(f\"üéÆ cuDNN versi√≥n: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'No disponible'}\")\n",
        "\n",
        "# Verificar CUDA\n",
        "print(\"üß™ Verificando CUDA...\")\n",
        "try:\n",
        "    x = torch.rand(5,3).cuda()\n",
        "    y = torch.matmul(x, x.t())\n",
        "    print(\"‚úÖ CUDA est√° funcionando correctamente\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al usar CUDA: {e}\")\n",
        "\n",
        "# Verificar dependencias cr√≠ticas\n",
        "print(\"üì¶ Verificando dependencias cr√≠ticas...\")\n",
        "try:\n",
        "    import huggingface_hub\n",
        "    print(f\"‚úÖ huggingface_hub: {huggingface_hub.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå huggingface_hub: {e}\")\n",
        "\n",
        "try:\n",
        "    import onnxruntime\n",
        "    print(f\"‚úÖ onnxruntime: {onnxruntime.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå onnxruntime: {e}\")\n",
        "\n",
        "# Verificar dependencias principales\n",
        "print(\"üîß Verificando dependencias principales...\")\n",
        "try:\n",
        "    from diffusers import __version__ as diffusers_version\n",
        "    print(f\"‚úÖ diffusers: {diffusers_version}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå diffusers: {e}\")\n",
        "\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"‚úÖ transformers: {transformers.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå transformers: {e}\")\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "    print(f\"‚úÖ opencv-python: {cv2.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå opencv-python: {e}\")\n",
        "\n",
        "try:\n",
        "    import insightface\n",
        "    print(f\"‚úÖ insightface: {insightface.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå insightface: {e}\")\n",
        "\n",
        "try:\n",
        "    import controlnet_aux\n",
        "    print(f\"‚úÖ controlnet_aux: {controlnet_aux.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå controlnet_aux: {e}\")\n",
        "\n",
        "try:\n",
        "    import gradio as gr\n",
        "    print(f\"‚úÖ gradio: {gr.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå gradio: {e}\")\n",
        "\n",
        "try:\n",
        "    import psutil\n",
        "    print(f\"‚úÖ psutil: {psutil.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå psutil: {e}\")\n",
        "\n",
        "print(\"üéØ Verificaci√≥n de compatibilidad:\")\n",
        "\n",
        "# Verificar compatibilidad de versiones (sin versiones fijas)\n",
        "def check_version_compatibility():\n",
        "    \"\"\"Verifica que las versiones sean compatibles con PyTorch 2.6+.\"\"\"\n",
        "    issues = []\n",
        "    \n",
        "    # Verificar PyTorch\n",
        "    torch_version = torch.__version__\n",
        "    major, minor = torch_version.split('.')[:2]\n",
        "    if int(major) < 2 or (int(major) == 2 and int(minor) < 0):\n",
        "        issues.append(f\"PyTorch {torch_version} puede ser muy antiguo\")\n",
        "    else:\n",
        "        print(f\"‚úÖ PyTorch {torch_version} - Compatible\")\n",
        "    \n",
        "    # Verificar transformers\n",
        "    try:\n",
        "        transformers_version = transformers.__version__\n",
        "        major, minor = transformers_version.split('.')[:2]\n",
        "        if int(major) < 4 or (int(major) == 4 and int(minor) < 41):\n",
        "            issues.append(f\"Transformers {transformers_version} puede ser incompatible (recomendado >=4.41.0)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Transformers {transformers_version} - Compatible\")\n",
        "    except:\n",
        "        issues.append(\"No se pudo verificar versi√≥n de transformers\")\n",
        "    \n",
        "    # Verificar diffusers\n",
        "    try:\n",
        "        diffusers_version_parts = diffusers_version.split('.')\n",
        "        \n",
        "        major, minor = int(diffusers_version_parts[0]), int(diffusers_version_parts[1])\n",
        "        if major < 0 or (major == 0 and minor < 30):\n",
        "            issues.append(f\"Diffusers {diffusers_version} puede ser incompatible (recomendado >=0.30.0)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Diffusers {diffusers_version} - Compatible\")\n",
        "    except:\n",
        "        issues.append(\"No se pudo verificar versi√≥n de diffusers\")\n",
        "    \n",
        "    return issues\n",
        "\n",
        "compatibility_issues = check_version_compatibility()\n",
        "\n",
        "if compatibility_issues:\n",
        "    print(\"‚ö†Ô∏è Posibles problemas de compatibilidad:\")\n",
        "    for issue in compatibility_issues:\n",
        "        print(f\"   ‚Ä¢ {issue}\")\n",
        "    print(\"üí° Si encuentras errores, considera reinstalar las dependencias.\")\n",
        "else:\n",
        "    print(\"‚úÖ Todas las versiones son compatibles!\")\n",
        "\n",
        "print(\"üöÄ El entorno est√° listo para usar InstantID!\")\n",
        "print(\"üìù Nota: Este notebook usa versiones compatibles con PyTorch 2.6+\")\n",
        "print(\"üìù No se requieren versiones espec√≠ficas fijas - se adapta al entorno actual\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa44c427",
      "metadata": {},
      "source": [
        "## Importar Dependencias\n",
        "Ejecuta esta celda despu√©s de reiniciar el entorno de ejecuci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2244b0df",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import transformers\n",
        "from diffusers import __version__ as diffusers_version\n",
        "from diffusers.utils import load_image\n",
        "from diffusers.models import ControlNetModel\n",
        "from insightface.app import FaceAnalysis\n",
        "# Importar el pipeline desde el repositorio clonado\n",
        "from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Verificar que todo est√° correcto\n",
        "print(f\"PyTorch CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Dispositivo CUDA: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memoria GPU total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "print(f\"\\nVersiones de las dependencias:\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"transformers: {transformers.__version__}\")\n",
        "print(f\"diffusers: {diffusers_version}\")\n",
        "print(f\"torch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a7baad",
      "metadata": {},
      "source": [
        "## Configuraci√≥n de Modelos\n",
        "En esta secci√≥n vamos a:\n",
        "1. Descargar los modelos necesarios\n",
        "2. Configurar el pipeline de InstantID\n",
        "3. Preparar el analizador facial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89c9a88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß† Configuraci√≥n de Modelos con Optimizaci√≥n de Memoria\n",
        "print(\"üöÄ Iniciando carga optimizada de modelos InstantID...\")\n",
        "\n",
        "# Configurar optimizaciones de memoria\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import psutil\n",
        "\n",
        "# Configurar variables de entorno para optimizaci√≥n\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "os.environ['TORCH_HOME'] = './torch_cache'\n",
        "os.environ['HF_HOME'] = './hf_cache'\n",
        "os.environ['TRANSFORMERS_CACHE'] = './transformers_cache'\n",
        "\n",
        "def get_memory_info():\n",
        "    \"\"\"Obtiene informaci√≥n de memoria.\"\"\"\n",
        "    vm = psutil.virtual_memory()\n",
        "    info = {\n",
        "        'total': vm.total / (1024**3),\n",
        "        'available': vm.available / (1024**3),\n",
        "        'used': vm.used / (1024**3),\n",
        "        'percent': vm.percent\n",
        "    }\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory = torch.cuda.mem_get_info()\n",
        "        info['gpu_free'] = gpu_memory[0] / (1024**3)\n",
        "        info['gpu_total'] = gpu_memory[1] / (1024**3)\n",
        "    return info\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Limpia memoria del sistema y GPU.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "# Mostrar informaci√≥n inicial\n",
        "memory = get_memory_info()\n",
        "print(f\"üíæ RAM Total: {memory['total']:.2f} GB\")\n",
        "print(f\"üíæ RAM Disponible: {memory['available']:.2f} GB\")\n",
        "print(f\"üíæ RAM Usada: {memory['used']:.2f} GB ({memory['percent']:.1f}%)\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üéÆ VRAM Total: {memory['gpu_total']:.2f} GB\")\n",
        "    print(f\"üéÆ VRAM Libre: {memory['gpu_free']:.2f} GB\")\n",
        "\n",
        "# Verificar memoria suficiente\n",
        "if memory['available'] < 2.0:\n",
        "    print(\"‚ö†Ô∏è  Memoria baja. Considera reiniciar el runtime.\")\n",
        "    print(\"üí° Runtime ‚Üí Restart runtime\")\n",
        "\n",
        "# Configurar PyTorch para memoria eficiente\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.cuda.set_per_process_memory_fraction(0.8)  # Usar solo 80% de VRAM\n",
        "    cleanup_memory()\n",
        "\n",
        "print(f\"üì± Dispositivo: {device}\")\n",
        "print(f\"üî¢ Tipo de datos: {dtype}\")\n",
        "\n",
        "print(\"\\nüìÅ Creando directorios...\")\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints/ControlNetModel\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "print(\"\\nüì• Descargando modelos necesarios...\")\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_files = [\n",
        "    {\"filename\": \"ControlNetModel/config.json\", \"repo_id\": \"InstantX/InstantID\"},\n",
        "    {\"filename\": \"ControlNetModel/diffusion_pytorch_model.safetensors\", \"repo_id\": \"InstantX/InstantID\"},\n",
        "    {\"filename\": \"ip-adapter.bin\", \"repo_id\": \"InstantX/InstantID\"}\n",
        "]\n",
        "\n",
        "for file_info in model_files:\n",
        "    print(f\"Descargando {file_info['filename']}...\")\n",
        "    hf_hub_download(\n",
        "        repo_id=file_info['repo_id'],\n",
        "        filename=file_info['filename'],\n",
        "        local_dir=\"./checkpoints\",\n",
        "        resume_download=True\n",
        "    )\n",
        "\n",
        "print(\"\\nüß† Configurando analizador facial...\")\n",
        "import zipfile\n",
        "from insightface.app import FaceAnalysis\n",
        "import urllib.request\n",
        "import shutil\n",
        "\n",
        "# Descargar el modelo buffalo_l\n",
        "model_dir = os.path.abspath(\"./models\")\n",
        "model_name = \"buffalo_l\"\n",
        "model_file = os.path.join(model_dir, f\"{model_name}.zip\")\n",
        "\n",
        "if not os.path.exists(os.path.join(model_dir, model_name)):\n",
        "    print(f\"Descargando modelo {model_name}...\")\n",
        "    url = \"https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip\"\n",
        "    \n",
        "    temp_dir = os.path.join(model_dir, \"temp\")\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "    \n",
        "    urllib.request.urlretrieve(url, model_file)\n",
        "    with zipfile.ZipFile(model_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_dir)\n",
        "    \n",
        "    for file in os.listdir(temp_dir):\n",
        "        src = os.path.join(temp_dir, file)\n",
        "        dst = os.path.join(model_dir, file)\n",
        "        if os.path.exists(dst):\n",
        "            os.remove(dst)\n",
        "        shutil.move(src, dst)\n",
        "    \n",
        "    os.remove(model_file)\n",
        "    shutil.rmtree(temp_dir)\n",
        "    print(\"Modelo descargado y configurado correctamente.\")\n",
        "\n",
        "# Limpiar memoria antes de cargar modelos\n",
        "cleanup_memory()\n",
        "\n",
        "# Configurar tama√±o de detecci√≥n seg√∫n memoria disponible\n",
        "memory = get_memory_info()\n",
        "det_size = (320, 320) if memory['available'] < 3.0 else (640, 640)\n",
        "if memory['available'] < 3.0:\n",
        "    print(\"üîß Usando configuraci√≥n de memoria reducida\")\n",
        "\n",
        "print(\"\\nInicializando analizador facial...\")\n",
        "global app\n",
        "app = FaceAnalysis(name=model_name, root=model_dir, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "app.prepare(ctx_id=0, det_size=det_size)\n",
        "print(\"Analizador facial inicializado correctamente.\")\n",
        "\n",
        "# Limpiar memoria entre cargas\n",
        "cleanup_memory()\n",
        "\n",
        "print(\"\\nüéõÔ∏è  Cargando ControlNet...\")\n",
        "from diffusers.models import ControlNetModel\n",
        "\n",
        "global controlnet\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    'checkpoints/ControlNetModel',\n",
        "    torch_dtype=dtype,\n",
        "    use_safetensors=True,\n",
        "    low_cpu_mem_usage=True  # Optimizaci√≥n de memoria\n",
        ")\n",
        "\n",
        "# Limpiar memoria antes del pipeline principal\n",
        "cleanup_memory()\n",
        "\n",
        "# Verificar memoria antes de cargar el pipeline principal\n",
        "memory = get_memory_info()\n",
        "if memory['available'] < 1.0:\n",
        "    print(\"‚ùå Memoria insuficiente para cargar el pipeline principal\")\n",
        "    print(\"üí° Intenta reiniciar el runtime o usar Colab Pro\")\n",
        "else:\n",
        "    print(\"\\nüöÄ Cargando pipeline principal...\")\n",
        "    from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n",
        "\n",
        "    global pipe\n",
        "    pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        controlnet=controlnet,\n",
        "        torch_dtype=dtype,\n",
        "        safety_checker=None,\n",
        "        feature_extractor=None,\n",
        "        variant=\"fp16\" if device == \"cuda\" else None,\n",
        "        low_cpu_mem_usage=True,  # Optimizaci√≥n de memoria\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Aplicar optimizaciones de memoria espec√≠ficas\n",
        "    if device == \"cuda\":\n",
        "        print(\"üîß Aplicando optimizaciones GPU...\")\n",
        "        pipe.enable_model_cpu_offload()  # Mover modelos a CPU cuando no se usen\n",
        "        pipe.enable_vae_slicing()        # Procesar VAE en chunks\n",
        "        pipe.enable_vae_tiling()         # Procesar VAE en tiles\n",
        "        pipe.enable_attention_slicing()  # Reducir memoria de atenci√≥n\n",
        "        \n",
        "        # Configuraci√≥n adicional para memoria muy limitada\n",
        "        memory = get_memory_info()\n",
        "        if memory['available'] < 2.0:\n",
        "            pipe.enable_sequential_cpu_offload()  # Offload secuencial m√°s agresivo\n",
        "            print(\"üîß Modo de memoria ultra-conservador activado\")\n",
        "\n",
        "    print(\"\\nüîå Cargando IP-Adapter...\")\n",
        "    pipe.load_ip_adapter_instantid('checkpoints/ip-adapter.bin')\n",
        "\n",
        "    # Limpieza final\n",
        "    cleanup_memory()\n",
        "\n",
        "    print(\"\\nüéâ ¬°Todos los modelos han sido cargados correctamente!\")\n",
        "    \n",
        "    # Mostrar estado final de memoria\n",
        "    final_memory = get_memory_info()\n",
        "    print(f\"üìä Memoria final disponible: {final_memory['available']:.2f} GB\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"üìä VRAM final libre: {final_memory['gpu_free']:.2f} GB\")\n",
        "    \n",
        "    print(\"\\nVerificando variables globales:\")\n",
        "    print(f\"Pipeline disponible: {'pipe' in globals()}\")\n",
        "    print(f\"Analizador facial disponible: {'app' in globals()}\")\n",
        "    print(f\"ControlNet disponible: {'controlnet' in globals()}\")\n",
        "    \n",
        "    print(\"\\nüéØ ¬°Listo para generar im√°genes!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97685d5b",
      "metadata": {},
      "source": [
        "## Funci√≥n de Generaci√≥n de Im√°genes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b642fab8",
      "metadata": {},
      "source": [
        "## Interfaz Web con Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d5eded",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_image(face_image_path, prompt, negative_prompt=None, num_steps=30, identitynet_strength_ratio=0.80, adapter_strength_ratio=0.80):\n",
        "    \"\"\"Genera una imagen usando InstantID con correcci√≥n de dimensiones.\"\"\"\n",
        "    try:\n",
        "        logger.info(\"Iniciando generaci√≥n de imagen (versi√≥n corregida)...\")\n",
        "        logger.info(f\"Par√°metros: prompt='{prompt}', steps={num_steps}, identity_strength={identitynet_strength_ratio}, adapter_strength={adapter_strength_ratio}\")\n",
        "        \n",
        "        if negative_prompt is None:\n",
        "            negative_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry\"\n",
        "        \n",
        "        logger.info(\"Cargando imagen...\")\n",
        "        face_image = load_image(face_image_path)\n",
        "        logger.info(\"Imagen cargada correctamente\")\n",
        "        \n",
        "        logger.info(\"Convirtiendo imagen para detecci√≥n facial...\")\n",
        "        face_image_cv2 = cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR)\n",
        "        \n",
        "        logger.info(\"Detectando rostro...\")\n",
        "        face_info = app.get(face_image_cv2)\n",
        "        if len(face_info) == 0:\n",
        "            raise ValueError(\"No se detect√≥ ning√∫n rostro en la imagen\")\n",
        "        logger.info(\"Rostro detectado correctamente\")\n",
        "        \n",
        "        face_info = face_info[-1]\n",
        "        face_emb = face_info['embedding']\n",
        "        face_kps = draw_kps(face_image, face_info['kps'])\n",
        "        logger.info(\"Puntos faciales extra√≠dos correctamente\")\n",
        "        \n",
        "        # üîß CORRECCI√ìN: Validar dimensiones antes de usar\n",
        "        if hasattr(face_kps, 'size'):\n",
        "            width, height = face_kps.size\n",
        "            logger.info(f\"Dimensiones originales: {width}x{height}\")\n",
        "        else:\n",
        "            width, height = 1024, 1024\n",
        "            logger.info(\"Usando dimensiones por defecto: 1024x1024\")\n",
        "        \n",
        "        # Asegurar que sean m√∫ltiplos de 8 y no None\n",
        "        width = max(512, (int(width) // 8) * 8) if width else 1024\n",
        "        height = max(512, (int(height) // 8) * 8) if height else 1024\n",
        "        \n",
        "        logger.info(f\"Dimensiones finales: {width}x{height}\")\n",
        "        \n",
        "        logger.info(\"Configurando par√°metros del pipeline...\")\n",
        "        pipe.set_ip_adapter_scale(adapter_strength_ratio)\n",
        "        \n",
        "        logger.info(\"Generando imagen...\")\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            image_embeds=face_emb,\n",
        "            image=face_kps,\n",
        "            controlnet_conditioning_scale=float(identitynet_strength_ratio),\n",
        "            num_inference_steps=num_steps,\n",
        "            guidance_scale=5.0,\n",
        "            height=int(height),  # üîß Asegurar que sea entero\n",
        "            width=int(width)     # üîß Asegurar que sea entero\n",
        "        ).images[0]\n",
        "        \n",
        "        logger.info(\"¬°Generaci√≥n completada!\")\n",
        "        return image\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en generate_image: {str(e)}\")\n",
        "        logger.error(f\"Traza completa:\\\\n{traceback.format_exc()}\")\n",
        "        raise\n",
        "\n",
        "def process_image(image, prompt, num_steps, identitynet_strength, adapter_strength):\n",
        "    \"\"\"Procesa la imagen para la interfaz Gradio.\"\"\"\n",
        "    try:\n",
        "        logger.info(\"=== Iniciando nuevo procesamiento de imagen ===\")\n",
        "        logger.info(f\"Tipo de imagen recibida: {type(image)}\")\n",
        "        \n",
        "        # Verificar variables globales\n",
        "        if 'pipe' not in globals() or pipe is None:\n",
        "            raise RuntimeError(\"Pipeline no disponible\")\n",
        "        if 'app' not in globals() or app is None:\n",
        "            raise RuntimeError(\"Analizador facial no disponible\")\n",
        "        \n",
        "        logger.info(\"Verificando estado de variables globales...\")\n",
        "        logger.info(f\"Pipeline disponible: {pipe is not None}\")\n",
        "        logger.info(f\"Analizador facial disponible: {app is not None}\")\n",
        "        logger.info(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "        if torch.cuda.is_available():\n",
        "            logger.info(f\"Memoria GPU disponible: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")\n",
        "        \n",
        "        # Guardar imagen temporal\n",
        "        temp_path = \"temp_face.png\"\n",
        "        if isinstance(image, str):\n",
        "            temp_path = image\n",
        "            logger.info(f\"Usando ruta de imagen existente: {temp_path}\")\n",
        "        else:\n",
        "            logger.info(\"Guardando imagen temporal...\")\n",
        "            image.save(temp_path)\n",
        "            logger.info(f\"Imagen guardada en: {temp_path}\")\n",
        "        \n",
        "        try:\n",
        "            logger.info(\"Llamando a generate_image...\")\n",
        "            result = generate_image(\n",
        "                face_image_path=temp_path,\n",
        "                prompt=prompt,\n",
        "                num_steps=int(num_steps),\n",
        "                identitynet_strength_ratio=float(identitynet_strength),\n",
        "                adapter_strength_ratio=float(adapter_strength)\n",
        "            )\n",
        "            logger.info(\"Imagen generada exitosamente\")\n",
        "            return result\n",
        "        \n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error durante la generaci√≥n: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            logger.error(f\"Traza completa:\\n{traceback.format_exc()}\")\n",
        "            # Retornar None en lugar de gr.Image.update que no existe\n",
        "            return None\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error en el procesamiento: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        logger.error(f\"Traza completa:\\n{traceback.format_exc()}\")\n",
        "        # Retornar None en lugar de gr.Image.update que no existe\n",
        "        return None\n",
        "        \n",
        "    finally:\n",
        "        # Limpiar archivo temporal\n",
        "        if isinstance(image, Image.Image) and os.path.exists(temp_path):\n",
        "            try:\n",
        "                os.remove(temp_path)\n",
        "                logger.info(\"Archivo temporal eliminado\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"No se pudo eliminar el archivo temporal: {str(e)}\")\n",
        "\n",
        "# Crear la interfaz\n",
        "demo = gr.Interface(\n",
        "    fn=process_image,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Imagen del rostro\"),\n",
        "        gr.Textbox(\n",
        "            label=\"Prompt\",\n",
        "            value=\"analog film photo of a person in a cyberpunk city, neon lights, cinematic lighting\"\n",
        "        ),\n",
        "        gr.Slider(minimum=20, maximum=100, value=30, step=1, label=\"N√∫mero de pasos\"),\n",
        "        gr.Slider(minimum=0.0, maximum=1.5, value=0.8, step=0.05, label=\"Fuerza IdentityNet\"),\n",
        "        gr.Slider(minimum=0.0, maximum=1.5, value=0.8, step=0.05, label=\"Fuerza Adapter\")\n",
        "    ],\n",
        "    outputs=gr.Image(type=\"pil\", label=\"Imagen generada\"),\n",
        "    title=\"InstantID - Generaci√≥n de Im√°genes\",\n",
        "    description=\"Sube una imagen con un rostro claro y visible, ajusta los par√°metros y genera una nueva imagen manteniendo la identidad.\"\n",
        ")\n",
        "\n",
        "logger.info(\"Interfaz Gradio inicializada. Lista para procesar im√°genes.\")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a694b6e",
      "metadata": {},
      "source": [
        "## Limpieza de Memoria"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "correction_info",
      "metadata": {},
      "source": [
        "## üîß Correcci√≥n Aplicada\\n",
        "\\n",
        "**Se ha aplicado una correcci√≥n autom√°tica** para solucionar el error:\\n",
        "```\\n",
        "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\\n",
        "```\\n",
        "\\n",
        "### Cambios realizados:\\n",
        "- ‚úÖ Validaci√≥n de dimensiones en `generate_image()`\\n",
        "- ‚úÖ Valores por defecto para height/width\\n",
        "- ‚úÖ Asegurar m√∫ltiplos de 8 (requerido por VAE)\\n",
        "- ‚úÖ Conversi√≥n expl√≠cita a enteros\\n",
        "\\n",
        "La interfaz web ahora deber√≠a funcionar sin errores de dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6181b58d",
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"Memoria GPU liberada\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}